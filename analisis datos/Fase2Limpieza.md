

**√öltima edici√≥n:** 2025-08-29 23:56

# Fase 2: Limpieza y Transformaci√≥n de Datos
‚úÖ Orden Limpieza y Transformaci√≥n de Datos (para An√°lisis y IA)
# 1. Revisi√≥n inicial

* Identificar dimensiones (filas, columnas, tipos de variables).
```
import pandas as pd

    # 1. Dimensiones del dataset
    print("üìä Dimensiones del dataset:")
    print(f"Filas: {df.shape[0]}")
    print(f"Columnas: {df.shape[1]}\n")

    # 2. Tipos de variables
    print("üìù Tipos de variables:")
    print(df.dtypes)

    # 3. Opcional: Resumen r√°pido con valores no nulos
    print("\nüîé Resumen con valores no nulos:")
    print(df.info())


```

* Verificar nombres de columnas.
```
    print("üìå Nombres originales de columnas:")
    print(df.columns.tolist())
```


* Detectar duplicados en filas o IDs.

```
    duplicados_filas = df.duplicated().sum()
    print(f"üìå N√∫mero de filas duplicadas: {duplicados_filas}")
    
    # Elimina filas duplicadas manteniendo la primera ocurrencia
    df_sin_dups = df.drop_duplicates()

```
* Explorar valores √∫nicos en variables categ√≥ricas.
```
# 1. Seleccionar solo columnas categ√≥ricas (tipo 'object' o 'category')
categoricas = df.select_dtypes(include=["object", "category"]).columns

 2. Explorar valores √∫nicos en cada columna categ√≥rica
for col in categoricas:
    print(f"\nüîé Columna: {col}")
    print(f"- N√∫mero de categor√≠as √∫nicas: {df[col].nunique()}")
    print(f"- Categor√≠as: {df[col].unique()[:10]}")  # Muestra solo las 10 primeras
    
```

# 2. Tratamiento de valores faltantes

MCAR (Missing Completely At Random): faltan ‚Äúal azar total‚Äù. Imputaciones simples suelen ser seguras.

MAR (Missing At Random): la ausencia depende de otras variables observadas (p. ej., los ingresos faltan m√°s en gente joven). M√©todos multivariados (KNN, MICE, modelos) funcionan mejor.

MNAR (Missing Not At Random): la ausencia depende del valor en s√≠ (p. ej., personas con ingresos muy altos no reportan). Es el m√°s dif√≠cil; requiere supuestos y, si es posible, recolectar m√°s datos o usar modelado expl√≠cito de la ausencia (indicadores, modelos de selecci√≥n).

* Calcular porcentaje de NaN/Null.
```
# 1. Calcular porcentaje de NaN/Null por columna
porcentaje_nulos = (df.isnull().sum() / len(df)) * 100

# 2. Crear un DataFrame ordenado con el resultado
reporte_nulos = porcentaje_nulos.reset_index()
reporte_nulos.columns = ["columna", "porcentaje_nulos"]
reporte_nulos = reporte_nulos.sort_values(by="porcentaje_nulos", ascending=False)

print("üìä Porcentaje de valores NaN/Null por columna:")
print(reporte_nulos)


```
visualizar valores faltantes 
```
    # df es tu DataFrame de Pandas con datos faltantes
    msno.bar(df)           # Gr√°fico de barras de missing
    msno.matrix(df)        # Matriz de valores faltantes
    msno.heatmap(df)       # Heatmap de correlaciones de missing
    msno.dendrogram(df)    # Dendrograma de patrones de valores faltantes

```
* Decidir: eliminar o imputar (media, mediana, moda, interpolaci√≥n, KNN, modelos, etc.).

* Convertir faltantes impl√≠citos ‚Üí expl√≠citos (NaN)

¬øQu√© es ‚Äúimpl√≠cito‚Äù? Celdas vac√≠as, "NA", "N/A", "None", "?", "‚Äî", "Sin dato", 0 √≥ 9999 usados como ‚Äúfaltante‚Äù, etc.

```
M√©todos de imputaci√≥n

import numpy as np

placeholders = ["NA","N/A","na","n/a","None","none","-","‚Äî","?", "", "Sin dato", "sd"]
df = df.replace(placeholders, np.nan)

# Si hay sentinelas num√©ricos:
sentinelas = { "edad": [0, 999], "ingreso": [0, 9999999] }
for col, vals in sentinelas.items():
    df[col] = df[col].replace(vals, np.nan)

```
### M√©todos de imputaci√≥n





| M√©todo                   | ¬øCu√°ndo usar?                             | Ventajas                                         | Desventajas                                                                            | Notas/Implementaci√≥n                                        |        |        |
| ------------------------ | ----------------------------------------- | ------------------------------------------------ | -------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------ | ------ |
| Media/Mediana/Moda       | MCAR o MAR leve; baseline r√°pido          | Simple, r√°pido, reproducible                     | Distorsiona varianza; media sensible a outliers; moda puede crear sesgo en categ√≥ricas | `SimpleImputer`; mediana para sesgo/outliers                |        |        |
| ffill/bfill              | Series temporales con continuidad         | Respeta tendencia local                          | Propaga errores; malo con huecos largos                                                | Ordenar por tiempo; combinar `ffill().bfill()`              |        |        |
| Interpolaci√≥n            | Se√±al ‚Äúsuave‚Äù (temperatura, sensores)     | Coherente con continuidad                        | Puede sobreajustar (spline); no sirve en categ√≥ricas                                   | \`interpolate(method="linear"                               | "time" | ...)\` |
| KNN Imputer              | MAR; relaciones multivariadas             | Captura relaciones entre features                | Costoso; sensible a escala y ruido                                                     | Estandariza antes; elegir `k` y distancia                   |        |        |
| Modelos (RF, GBM, etc.)  | MAR fuerte; se√±al predictiva alta         | Flexible, no lineal                              | Complejo; riesgo de leakage si no se cuida                                             | Entrenar **solo con train**; repetir por variable objetivo  |        |        |
| MICE (IterativeImputer)  | MAR multivariado; datasets medianos       | Usa todas las relaciones; m√∫ltiples imputaciones | Lento; tuning/convergencia                                                             | `IterativeImputer` (experimental); puede samplear posterior |        |        |
| Eliminar filas/columnas  | Alto % de faltantes y/o poca importancia  | Sencillo, evita supuestos                        | Pierdes informaci√≥n; posible sesgo                                                     | Usa umbral y criterio de valor anal√≠tico                    |        |        |
| Transformaci√≥n + inversa | Variables muy sesgadas (p. ej., ingresos) | Mejora supuestos de m√©todos                      | A√±ade complejidad                                                                      | `log1p` ‚Üí imputar ‚Üí `expm1` para revertir                   |        |        |



#### Media / Mediana / Moda (r√°pidos)

* ¬øC√≥mo funciona?

    *   Media: sustituye valores faltantes por la media de la columna.
       
    *   Mediana: sustituye por el valor central (m√°s robusta a outliers).
       
    *   Moda: sustituye con la categor√≠a m√°s frecuente (para variables categ√≥ricas).

* üìà Cu√°ndo usar

    * Datos MCAR (faltan completamente al azar).

    *   Columnas con baja proporci√≥n de faltantes (< 20%).
       
    *   Variables num√©ricas con distribuci√≥n no muy sesgada (media) o sesgada/outliers (mediana).

* ‚úÖ Ventajas

    * Muy r√°pido y f√°cil de aplicar.
    
    * Baseline ideal antes de probar m√©todos m√°s complejos.

* ‚ùå Desventajas

    *Reduce la varianza ‚Üí subestima la dispersi√≥n.
    
    *Puede introducir sesgo si los datos no son MCAR.
    
    *No captura relaciones entre variables.

```
from sklearn.impute import SimpleImputer

# Num√©ricas ‚Üí mediana
imp_mediana = SimpleImputer(strategy="median")
df["edad"] = imp_mediana.fit_transform(df[["edad"]])

# Categ√≥ricas ‚Üí moda
imp_moda = SimpleImputer(strategy="most_frequent")
df["pais"] = imp_moda.fit_transform(df[["pais"]])
```

#### Llenado hacia atr√°s / hacia adelante (series de tiempo)

* ¬øC√≥mo funciona?

ffill (forward fill): copia el √∫ltimo valor v√°lido hacia adelante.

bfill (backward fill): copia el siguiente valor v√°lido hacia atr√°s.

* üìà Cu√°ndo usar

Series de tiempo o datos ordenados.

Variables que cambian lentamente en el tiempo (ej. temperatura, saldos).

* ‚úÖ Ventajas

Conserva la tendencia local.

Muy eficiente en datos secuenciales.

* ‚ùå Desventajas

Propaga errores si el √∫ltimo dato es incorrecto.

No funciona con huecos muy largos.

```
df = df.sort_values("fecha")
df["temperatura"] = df["temperatura"].ffill().bfill()
```
#### Interpolaci√≥n

* üîß ¬øC√≥mo funciona?

Estima los valores faltantes en base a valores vecinos.

M√©todos comunes: linear, time, spline, polynomial.

 `method="linear"` (por defecto)
- Rellena los valores faltantes con una l√≠nea recta entre los puntos conocidos.  
- Ejemplo: si tienes `2` y `6` con un `NaN` en medio ‚Üí lo rellena con `4`.  
- Es el m√°s usado y r√°pido.  

---

 `method="time"`
- Se usa si el √≠ndice es de tipo fecha (`DatetimeIndex`).  
- Hace interpolaci√≥n lineal tomando en cuenta el tiempo transcurrido.  
- √ötil en series temporales.  

---

 `method="index"`
- Similar a `linear`, pero usa los valores del √≠ndice num√©rico como referencia para calcular la interpolaci√≥n.  

---

 `method="nearest"`
- Rellena usando el valor del punto m√°s cercano.  
- No genera valores intermedios, solo copia.  

---

`method="spline", order=n`
- Usa polinomios (curvas suaves) para estimar valores.  
- `order=2` ‚Üí cuadr√°tica.  
- `order=3` ‚Üí c√∫bica.  
- M√°s preciso para datos con curvatura, pero m√°s lento.  

---
 `method="polynomial", order=n`
- Similar a `spline`, pero ajusta un polinomio global a los datos.  
- Puede sobreajustar si los datos son muy ruidosos.  

---

 `method="pad"` o `method="ffill"` (forward fill)
- Copia el √∫ltimo valor v√°lido hacia adelante.  
- √ötil en datos categ√≥ricos o de sensores.  

---

`method="bfill"` (backward fill)
- Copia el siguiente valor v√°lido hacia atr√°s.  
- Tambi√©n se usa en datos categ√≥ricos.  

---


* üìà Cu√°ndo usar

Series de tiempo o procesos continuos/suaves.

Variables f√≠sicas (ej. sensores, clima).

* ‚úÖ Ventajas

M√°s realista que ffill/bfill.

Admite interpolaciones avanzadas (polinomiales, spline).

* ‚ùå Desventajas

Puede sobreajustar si los datos no son lineales.

No sirve para categ√≥ricas.

#### KNN Imputer

üîß ¬øC√≥mo funciona?

Para cada valor faltante, busca sus k vecinos m√°s cercanos (por distancia en otras variables) y asigna:

La media (num√©ricas).

La moda (categ√≥ricas).

üìà Cu√°ndo usar

Datos MAR (faltan en funci√≥n de otras variables).

Variables con correlaci√≥n fuerte con otras.

‚úÖ Ventajas

Captura relaciones multivariadas.

Puede ser m√°s preciso que m√©todos univariados.

‚ùå Desventajas

Muy costoso en datasets grandes.

Sensible al escalado (normalizar antes).

Si hay mucho ruido, puede imputar mal.

```
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5, weights="distance")
df_imputed = imputer.fit_transform(df)
```
####  Imputaci√≥n basada en Modelos predictivos
üîß ¬øC√≥mo funciona?

Entrena un modelo (ej. regresi√≥n lineal, Random Forest, XGBoost) para predecir la variable con faltantes en funci√≥n de las dem√°s.

üìà Cu√°ndo usar

Variables con mucha relaci√≥n con otras.

Cuando los datos faltan bajo MAR.

‚úÖ Ventajas

Captura relaciones complejas.

Puede mejorar la calidad de la imputaci√≥n.

‚ùå Desventajas

M√°s costoso en tiempo y recursos.

Riesgo de data leakage si no se divide correctamente.

Puede introducir sobreajuste.

```
from sklearn.ensemble import RandomForestRegressor

train = df[df["ingreso"].notna()]
test  = df[df["ingreso"].isna()]

X_train = train.drop("ingreso", axis=1)
y_train = train["ingreso"]

model = RandomForestRegressor()
model.fit(X_train, y_train)

df.loc[df["ingreso"].isna(), "ingreso"] = model.predict(test.drop("ingreso", axis=1))
```

####   MICE (Multiple Imputation by Chained Equations)
üîß ¬øC√≥mo funciona?

Imputa iterativamente cada variable faltante en funci√≥n de las dem√°s.

Repite varias rondas ‚Üí convergencia.

Permite generar m√∫ltiples datasets imputados para tener en cuenta la incertidumbre.

üìà Cu√°ndo usar

Datos con varios campos faltantes.

MAR y correlaciones complejas entre variables.

‚úÖ Ventajas

Considera relaciones multivariadas.

Puede mejorar modelos finales.

Permite estimar incertidumbre (estad√≠stica inferencial).

‚ùå Desventajas

Costoso y lento en grandes datasets.

Implementaci√≥n m√°s compleja.

```
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp = IterativeImputer(max_iter=10, random_state=42)
df_imputed = imp.fit_transform(df)

``` 

####  Transformaci√≥n inversa de datos
üîß ¬øC√≥mo funciona?

Se transforma la variable (ej. logaritmo, Box-Cox) para estabilizar la distribuci√≥n.

Se imputa en el espacio transformado.

Se aplica la inversa para recuperar la escala original.

üìà Cu√°ndo usar

Variables muy sesgadas (ej. ingresos, precios, ventas).

‚úÖ Ventajas

Hace que imputaciones simples (media/mediana) sean m√°s realistas.

Reduce sesgos por distribuciones muy largas.

‚ùå Desventajas

Requiere aplicar correctamente la inversa.

No todas las transformaciones son adecuadas.

```
import numpy as np

df["ingreso_log"] = np.log1p(df["ingreso"])
df["ingreso_log"] = df["ingreso_log"].fillna(df["ingreso_log"].median())
df["ingreso"] = np.expm1(df["ingreso_log"])
```





# 3. Estandarizaci√≥n de tipos de datos

* Convertir fechas a datetime.
```
pd.to_datetime(["2024-05-10", "10/05/2024"], dayfirst=True)
gumentos m√°s √∫tiles:

errors: qu√© hacer si no puede convertir.

"raise" ‚Üí error.

"coerce" ‚Üí pone NaT (Not a Time).

"ignore" ‚Üí deja el valor como estaba.

dayfirst=True: interpreta formato DD/MM/YYYY.

format: le dices el formato exacto ("%d-%m-%Y", "%Y/%m/%d %H:%M").

```
| Conjunto       | % t√≠pico | Uso principal                                 | ¬øSe entrena con √©l? | ¬øSe ajusta con √©l? |
| -------------- | -------- | --------------------------------------------- | ------------------- | ------------------ |
| **Train**      | 60‚Äì80%   | Aprender patrones, entrenar modelo            | ‚úÖ S√≠                | ‚ùå No               |
| **Validation** | 10‚Äì20%   | Ajustar hiperpar√°metros, prevenir overfitting | ‚ùå No                | ‚úÖ S√≠               |
| **Test**       | 10‚Äì20%   | Evaluaci√≥n final del modelo                   | ‚ùå No                | ‚ùå No               |


* Variables categ√≥ricas ‚Üí tipo category.
    Regla general
    Primero se divide el dataset en entrenamiento y prueba.
    Despu√©s se ajusta (fit) la codificaci√≥n solo con el conjunto de entrenamiento.
    Luego se aplica la misma transformaci√≥n al conjunto de prueba (transform).

* Variables num√©ricas ‚Üí asegurar int o float.
```
    df["edad"] = pd.to_numeric(df["edad"], errors="coerce").astype("Int64")

```

* Normalizar texto (min√∫sculas, sin espacios extra, sin caracteres raros).
 ```
    str(x).lower().strip()
 ```
* Outliers evidentes (errores de captura) ‚Üí limpiar aqu√≠.

# 4. Divisi√≥n en conjuntos (opcional si se entrena/valida en el mismo dataset)

Train / Test (y validaci√≥n si aplica).
``` 
# Dividir en entrenamiento (80%) y prueba (20%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# opcional 
# Luego dividir el temporal en Validation/Test (50%-50%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

```


A partir de aqu√≠, todas las transformaciones que dependen de c√°lculos se hacen con fit en train, transform en test.

# 5. Tratamiento de valores at√≠picos (outliers genuinos)

M√©todos: IQR, Z-score, boxplots.

Opciones: winsorizaci√≥n, escalamiento robusto, mantenerlos si son v√°lidos.

# 6. Transformaciones dependientes de estad√≠sticos

Normalizaci√≥n / Escalado (MinMax, StandardScaler, RobustScaler).

Codificaci√≥n de categ√≥ricas (OneHot, Label, Target Encoding ‚Üí con fit en train).

Reducci√≥n de dimensionalidad (PCA, UMAP, etc.).

# 7. Feature Engineering (creaci√≥n de nuevas variables)

Derivadas de tiempo (a√±o, mes, d√≠a, hora).

Interacciones (ratios, logaritmos, diferencias).

Texto: TF-IDF, embeddings.

Im√°genes: normalizaci√≥n de p√≠xeles, extracci√≥n de features.

# 8. Balanceo de clases (si es clasificaci√≥n)

Oversampling (SMOTE), undersampling o pesos de clase.

Siempre solo en el conjunto de entrenamiento.

# 9. Validaci√≥n de consistencia

Revisar correlaciones entre variables.

Revisar coherencia temporal (fechas de inicio < fechas de fin).

Verificar que train/test tienen distribuciones similares (no hay ‚Äúdrift‚Äù).

# 10. Guardar dataset limpio

data/raw ‚Üí dataset original.

data/processed ‚Üí datasets limpios (train y test transformados).

Documentar decisiones (log de transformaciones).