{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bf05b3",
   "metadata": {},
   "source": [
    "# Fase 2: Limpieza y Transformación de Datos\n",
    "\n",
    "Objetivo: Corregir los problemas identificados en la Fase 1 y enriquecer el dataset para prepararlo para el modelado. La calidad y el rendimiento de tu futuro modelo dependen casi por completo de la calidad de esta fase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681d84f",
   "metadata": {},
   "source": [
    "### Manejo de nulos\n",
    "* **Código de Ejemplo:** `df['col'].fillna(df['col'].median())` o `df.dropna()`\n",
    "* **Consejo Profesional:** Usa la mediana si hay valores atípicos (outliers). Eliminar filas o columnas es el último recurso.\n",
    "\n",
    "---\n",
    "\n",
    "### Conversión de tipos\n",
    "* **Código de Ejemplo:** `df['fecha'] = pd.to_datetime(df['fecha'])`\n",
    "* **Consejo Profesional:** Los identificadores (IDs) numéricos deberían tratarse como texto (strings) para evitar cálculos incorrectos.\n",
    "\n",
    "---\n",
    "\n",
    "### Eliminar duplicados\n",
    "* **Código de Ejemplo:** `df.drop_duplicates(inplace=True)`\n",
    "* **Consejo Profesional:** Revisa los duplicados antes de eliminarlos, ya que su presencia puede indicar errores en el proceso de recolección de datos.\n",
    "\n",
    "---\n",
    "\n",
    "### Detección y tratamiento de outliers\n",
    "* **Código de Ejemplo:**\n",
    "    ```python\n",
    "    Q1 = df['col'].quantile(0.25)\n",
    "    Q3 = df['col'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[df['col'] < Q3 + 1.5*IQR]\n",
    "    ```\n",
    "* **Consejo Profesional:** Visualiza los datos con un `boxplot` de Seaborn (`sns.boxplot`). Otras técnicas incluyen la transformación logarítmica o la winsorización.\n",
    "\n",
    "---\n",
    "\n",
    "### Ingeniería de variables\n",
    "* **Código de Ejemplo:** `df['volumen_total'] = df['sets'] * df['reps']` o `df['dia_semana'] = df['fecha'].dt.dayofweek`\n",
    "* **Consejo Profesional:** En este paso es donde aplicas tu conocimiento del negocio para crear nuevas características relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "### Codificar categóricas\n",
    "* **Código de Ejemplo:** `pd.get_dummies(df, columns=['cat1'], drop_first=True)`\n",
    "* **Consejo Profesional:** Usa `drop_first=True` para evitar la multicolinealidad. Para variables con un orden inherente (ordinales), considera usar `OrdinalEncoder`.\n",
    "\n",
    "---\n",
    "\n",
    "### Escalado de datos\n",
    "* **Código de Ejemplo:**\n",
    "    ```python\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    ```\n",
    "* **Consejo Profesional:** También puedes considerar otras técnicas como `MinMaxScaler` (para escalar a un rango específico) o `RobustScaler` (si hay outliers).\n",
    "\n",
    "---\n",
    "\n",
    "### Guardar datos procesados\n",
    "* **Código de Ejemplo:** `df.to_csv('data/processed/datos_limpios.csv', index=False)`\n",
    "* **Consejo Profesional:** Usa `index=False` para evitar que el índice del DataFrame se guarde como una columna en el archivo CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20757453",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## [__] verificar nombres de columnas \n",
    "## [__] Identificar columnas con un solo valor\n",
    "## [__] datos a minuscula\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273d80d",
   "metadata": {},
   "source": [
    "## [__] 1. Eliminar duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25062f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [   ]Calcula el número de duplicados antes de eliminarlos.\n",
    "dups_before = df_clean.duplicated().sum()\n",
    "\n",
    "#[    ] Elimina las filas duplicadas, manteniendo la primera aparición.\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n",
    "#Consejo Profesional: A veces, las filas pueden estar duplicadas\n",
    "# solo en un subconjunto de columnas clave (ej., id_cliente y \n",
    "# fecha_compra). Puedes especificar estas columnas con el parámetro\n",
    "# subset: df_clean.drop_duplicates(subset=['id_cliente', \n",
    "# 'fecha_compra'], inplace=True).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b38699",
   "metadata": {},
   "source": [
    "## [__] Revisar duplicados parciales : mismo nombre y correo pero diferente ID\n",
    "## [__] Revisar claves primarias  ¿hay valores repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72049c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [__] 2. Eliminar o imputar valores nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [   ] --- Estrategia 1: Imputación para columnas numéricas ---\n",
    "# Si la distribución es sesgada (como viste en el histograma), la mediana es más robusta que la media.\n",
    "median_value = df_clean['columna_numerica_con_nulos'].median()\n",
    "df_clean['columna_numerica_con_nulos'].fillna(median_value, inplace=True)\n",
    "\n",
    "# [   ] --- Estrategia 2: Imputación para columnas categóricas ---\n",
    "# La moda (el valor más frecuente) es la mejor opción para imputar categorías.\n",
    "mode_value = df_clean['columna_categorica_con_nulos'].mode()[0]\n",
    "df_clean['columna_categorica_con_nulos'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# [   ] --- Estrategia 3: Eliminación de filas ---\n",
    "# Úsalo como último recurso si una fila tiene demasiados datos importantes faltantes.\n",
    "# 'subset' especifica la columna a revisar para la eliminación.\n",
    "df_clean.dropna(subset=['columna_clave_con_nulos'], inplace=True)\n",
    "\n",
    "#[  ]Consejo Profesional: La decisión de imputar o eliminar \n",
    "# depende del contexto del negocio y la cantidad de datos \n",
    "# faltantes. Si una columna tiene >50% de nulos, considera \n",
    "# eliminar la columna por completo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e454492",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [__] 3. Corrección de Tipos de Datos (Dtypes)\n",
    "Meta: Asegurarse de que cada columna tenga el tipo de dato correcto para poder realizar operaciones y análisis adecuados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[____] --- Convertir a tipo Fecha (datetime) ---\n",
    "# Es fundamental para poder realizar operaciones de series de tiempo.\n",
    "# 'errors='coerce'' convertirá las fechas no válidas en NaT (Not a Time),\n",
    "# que puedes manejar después.\n",
    "df_clean['columna_fecha'] = pd.to_datetime(df_clean['columna_fecha'], errors='coerce')\n",
    "\n",
    "\n",
    "# [___]--- Convertir a tipo Categórico (category) ---\n",
    "# Es más eficiente en memoria que el tipo 'object' para columnas con un número \n",
    "# limitado de valores únicos.\n",
    "df_clean['columna_a_categoria'] = df_clean['columna_a_categoria'].astype('category') \n",
    "\n",
    "\n",
    "# [___] --- Convertir a tipo Numérico (int, float) ---\n",
    "# Útil si tienes números almacenados como texto (ej. '$1,200.50').\n",
    "# Primero, necesitas limpiar los caracteres no numéricos.\n",
    "df_clean['columna_dinero'] = df_clean['columna_dinero'].replace({'\\$': '', ',': ''}, regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33f139",
   "metadata": {},
   "source": [
    "### [__] Eliminar simbolos no numericos\n",
    "### [__] Detectar valores negativos donde no deberia\n",
    "### [__] elimianr caracteres especiales si es posible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ea7b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [__] 4. Corregir formatos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = df['col'].str.strip().str.lower().str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1f808",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [___] 5. Manejo de Outliers en Análisis de Datos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5450a1",
   "metadata": {},
   "source": [
    "### [__] Detección de Outliers\n",
    "    Métodos:\n",
    "    --    * Boxplot / Gráficos\n",
    "       * Z-Score\n",
    "            ¿Qué es? Calcula cuántas desviaciones estándar se aleja un punto de la media. Un umbral común es un Z-score de +/- 3.\n",
    "            Ideal para: Datos que siguen una distribución normal (Gaussiana). Es sensible a los propios outliers que inflan la media y la desviación estándar.\n",
    "        * IQR\n",
    "                ¿Qué es? Un outlier es cualquier valor que cae fuera del siguiente rango: [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR].\n",
    "                Ideal para: Distribuciones asimétricas o cuando no quieres asumir una distribución normal. Es el método más común y robusto.\n",
    "        Distribución y visualización\n",
    "        MAD (desviación absoluta de la mediana)\n",
    "            se utiliza para medir la dispersión de un conjunto de datos, especialmente cuando se busca una medida robusta que no sea sensible a valores atípicos o extremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#[__] Visualizar\n",
    "sns.boxplot(data=df[['columna1', 'columna2']])\n",
    "plt.title('Boxplot para detectar outliers')\n",
    "plt.show()\n",
    "\n",
    "# [__] Z-Score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include='number')))\n",
    "df_outliers_z = df[(z_scores > 3).any(axis=1)]\n",
    "\n",
    "\n",
    "#[__] IQR\n",
    "Q1 = df['columna'].quantile(0.25)\n",
    "Q3 = df['columna'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lim_inf = Q1 - 1.5 * IQR\n",
    "lim_sup = Q3 + 1.5 * IQR\n",
    "outliers_iqr = df[(df['columna'] < lim_inf) | (df['columna'] > lim_sup)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96a4dd",
   "metadata": {},
   "source": [
    "### [__] ✂️ 2. Eliminación de Outliers\n",
    "\n",
    "    Cuándo usar:\n",
    "        Cuando el outlier es un error de medición evidente.\n",
    "        Cuando es poco probable que ese valor se repita.\n",
    "        Cuando distorsiona demasiado las métricas estadísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR para eliminar\n",
    "df_filtrado = df[(df['columna'] >= lim_inf) & (df['columna'] <= lim_sup)]\n",
    "# Z-score para eliminar\n",
    "df_filtrado = df[(z_scores < 3).all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb09a1",
   "metadata": {},
   "source": [
    "✅ Recomendaciones:\n",
    "*    Documenta cuántos valores se eliminaron.\n",
    "*    Evalúa el impacto en el tamaño del dataset.\n",
    "*    Guarda copia del dataset original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76141cc",
   "metadata": {},
   "source": [
    "### [___] 🔁 3. Reemplazo (Winsorización)\n",
    "\n",
    "📌 Objetivo: Sustituir outliers por valores extremos válidos sin perder la fila.\n",
    "\n",
    "* Qué es? Se \"aplanan\" los outliers. Cualquier valor por encima del percentil 95 (por ejemplo) se reemplaza por el valor del percentil 95. Lo mismo para el extremo inferior (ej. percentil 5).\n",
    "* Cuándo usarla? Cuando crees que los outliers son legítimos pero su magnitud extrema está afectando negativamente al modelo (especialmente modelos lineales). Conservas la idea de que es un valor \"alto\" o \"bajo\" sin que su escala domine.\n",
    "\n",
    "Cuándo usar:\n",
    "\n",
    "    Cuando quieres mantener todos los datos pero reducir la influencia de extremos.\n",
    "\n",
    "    Cuando el outlier puede ser real pero necesitas controlarlo estadísticamente.\n",
    "\n",
    "✅ Recomendaciones:\n",
    "\n",
    "    Winsorizar especialmente útil si vas a usar modelos sensibles (como regresión lineal).\n",
    "\n",
    "    Anota los límites usados (lim_inf, lim_sup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d410497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsoriza al 5% en cada extremo (deja el 90% de los datos centrales intactos)\n",
    "df['columna_winsorizada'] = winsorize(df['columna'], limits=[0.05, 0.05])\n",
    "\n",
    "df['columna'] = np.where(df['columna'] > lim_sup, lim_sup,\n",
    "                  np.where(df['columna'] < lim_inf, lim_inf, df['columna']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367d036",
   "metadata": {},
   "source": [
    "### [__] 🔄 4. Imputación de valores Mediana o Media\n",
    "\n",
    "📌 Objetivo: Sustituir outliers por valores estadísticos como la media o mediana.\n",
    "\n",
    "* ¿Qué es? Se sustituye el valor atípico por la mediana (más robusta) o la media de la columna.\n",
    "* ¿Cuándo usarla? Cuando no quieres perder los datos de las otras columnas de esa fila. Es un método simple y rápido. Prefiere la mediana sobre la media si la distribución es asimétrica.\n",
    "\n",
    "Cuándo usar:\n",
    "\n",
    "    Cuando los outliers son errores pero no puedes eliminar datos.\n",
    "\n",
    "    Si los datos son escasos o críticos y no quieres perder información\n",
    "\n",
    " Recomendaciones:\n",
    "\n",
    "    Usa la mediana en vez de la media si los datos están sesgados.\n",
    "\n",
    "    Documenta los valores imputados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediana = df['columna'].median()\n",
    "df['columna_imputada'] = df['columna'].apply(lambda x: mediana if x < limite_inferior or x > limite_superior else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283babd1",
   "metadata": {},
   "source": [
    "### [__] 🔁 5. Transformaciones de datos\n",
    "\n",
    "Aplicas una función matemática a toda la variable para reducir el impacto de los outliers.\n",
    "\n",
    "¿Qué es? Funciones como la logarítmica, raíz cuadrada o Box-Cox comprimen la escala de la variable, acercando los outliers al resto de los datos.\n",
    "\n",
    "¿Cuándo usarla?\n",
    "\n",
    "        Cuando la variable tiene una fuerte asimetría positiva (una cola larga a la derecha).\n",
    "\n",
    "        En modelos que asumen normalidad o linealidad (como la regresión lineal).\n",
    "\n",
    "        Cuando los outliers son una característica intrínseca de la distribución (ej. ingresos, tamaños de empresas).\n",
    "\n",
    "✅ Recomendaciones:\n",
    "\n",
    "    Usa log1p si hay ceros.\n",
    "\n",
    "    Visualiza después de transformar (histplot, boxplot).\n",
    "\n",
    "    No olvides destransformar al interpretar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logaritmo\n",
    "df['columna_log'] = np.log1p(df['columna'])\n",
    "\n",
    "# Raíz cuadrada\n",
    "df['columna_sqrt'] = np.sqrt(df['columna'])\n",
    "\n",
    "# Box-Cox (requiere valores > 0)\n",
    "from scipy.stats import boxcox\n",
    "df['columna_boxcox'], _ = boxcox(df['columna'] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac4d68",
   "metadata": {},
   "source": [
    "### [__] 🛡️ 6. Uso de Modelos Robustecidos\n",
    "📌 Objetivo: Modelar sin eliminar ni transformar outliers directamente.\n",
    "💡 Cuándo usar:\n",
    "\n",
    "    Cuando los outliers son inevitables.\n",
    "\n",
    "    Cuando usas modelos resistentes como árboles o medianas.\n",
    "\n",
    "💻 Modelos comunes:\n",
    "\n",
    "    RandomForest, XGBoost, GradientBoosting\n",
    "\n",
    "    HuberRegressor, RANSACRegressor (de sklearn.linear_model)\n",
    "\n",
    "    IsolationForest (para detección automática)\n",
    "\n",
    "Modelos Robustos:\n",
    "\n",
    "    Árboles de Decisión y ensambles (Random Forest, Gradient Boosting): Son naturalmente robustos a los outliers porque dividen el espacio de características en regiones y no se ven afectados por la magnitud de los valores.\n",
    "\n",
    "    Regresiones Robustas (ej. RANSAC, Huber Regressor): Modelos lineales que ponderan menos los errores grandes, reduciendo la influencia de los outliers.\n",
    "\n",
    "    Recomendación: Si eliges esta vía, simplemente alimenta los datos originales (o con mínima limpieza) a uno de estos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844cb44",
   "metadata": {},
   "source": [
    "### [__] 7. Análisis Individual o Validación del Outlier\n",
    "📌 Objetivo: Determinar si un outlier es realmente un error o un caso especial.\n",
    "💡 Cuándo usar:\n",
    "\n",
    "    Cuando un valor parece raro pero puede ser legítimo.\n",
    "\n",
    "    En áreas sensibles como medicina, fraude, accidentes, etc.\n",
    "\n",
    "✅ Recomendaciones:\n",
    "\n",
    "    Revisa el caso completo: ¿otros campos están bien?\n",
    "\n",
    "    Consulta con el dominio experto si es posible.\n",
    "\n",
    "    Nunca elimines automáticamente sin investigar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3425",
   "metadata": {},
   "source": [
    "### [___] 8. Documentación del tratamiento\n",
    "\n",
    "    🔎 ¿Cómo se detectaron?\n",
    "\n",
    "    🔁 ¿Qué método se aplicó?\n",
    "\n",
    "    📉 ¿Cuántos valores se afectaron?\n",
    "\n",
    "    🧾 ¿Qué decisiones se tomaron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780944a",
   "metadata": {},
   "source": [
    "---\n",
    "## [___] 6. Crear variables útiles (feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duracion_total'] = df['dias'] * df['horas_dia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007e0c5",
   "metadata": {},
   "source": [
    "| Técnica                          | Descripción breve                                            | Ejemplo de código                                                       | ¿Cuándo usar?                                             |\n",
    "| -------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| 🔢 Transformación matemática     | Aplica log, raíz, potencias para normalizar o ajustar escala | `df['log_precio'] = np.log1p(df['precio'])`                             | Cuando hay sesgo o colas largas                           |\n",
    "| ➗ Variables combinadas/derivadas | Combina columnas para crear relaciones útiles                | `df['precio_m2'] = df['precio'] / df['area']`                           | Cuando dos columnas están relacionadas                    |\n",
    "| 🔁 Encoding categórico           | Convierte texto a números                                    | `pd.get_dummies(df, columns=['genero'])`                                | Al usar modelos que requieren datos numéricos             |\n",
    "| 📆 Variables de fecha            | Extrae partes de una fecha o calcula duración                | `df['mes'] = df['fecha'].dt.month`                                      | Si tienes fechas (ventas, eventos, registros, etc.)       |\n",
    "| 🔢 Frecuencia o conteo           | Crea una nueva variable con la frecuencia de ocurrencia      | `df['freq'] = df['ciudad'].map(df['ciudad'].value_counts())`            | Para capturar importancia o rareza de categorías          |\n",
    "| 🧱 Binning / Discretización      | Convierte valores continuos a rangos                         | `pd.cut(df['edad'], bins=[0,18,35,60], labels=[...])`                   | Para segmentar poblaciones o estabilizar valores extremos |\n",
    "| ✅ Indicadores (flags)            | Crea columnas booleanas según condiciones                    | `df['es_vip'] = df['gasto'] > 100000`                                   | Para marcar eventos o atributos clave                     |\n",
    "| 📊 Estadísticas por grupo        | Calcula medias, desvíos por grupo                            | `df['media_ciudad'] = df.groupby('ciudad')['precio'].transform('mean')` | Para contextualizar los datos dentro de grupos            |\n",
    "| ✖️ Interacciones entre variables | Multiplica o combina variables para capturar relaciones      | `df['edad_x_ingresos'] = df['edad'] * df['ingresos']`                   | Cuando sospechas relaciones no lineales o sinérgicas      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558796a",
   "metadata": {},
   "source": [
    "----\n",
    "## [___] 7. Escalar datos (si es necesario)\n",
    "\n",
    "| Escenario                                                    | ¿Escalar?                                           | ¿Qué tipo?                      |\n",
    "| ------------------------------------------------------------ | --------------------------------------------------- | ------------------------------- |\n",
    "| Árboles (Decision Tree, Random Forest, XGBoost)              | ❌ No necesario                                      | Ninguno                         |\n",
    "| Regresiones lineales / logísticas                            | ✅ Sí                                                | Estandarización o normalización |\n",
    "| Modelos de ML sensibles a distancia (KNN, SVM, PCA, K-means) | ✅ Sí                                                | Recomendado                     |\n",
    "| Redes neuronales (MLP, Deep Learning)                        | ✅ Imprescindible                                    | Normalización (0-1)             |\n",
    "| Datos categóricos codificados (one-hot)                      | ❌ No                                                | -                               |\n",
    "| Datos con outliers                                           | ⚠️ Escalar con RobustScaler o usar métodos robustos | Sí                              |\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "| Escalador          | ¿Cómo funciona?                            | Cuándo usarlo                  | Código                             |\n",
    "| ------------------ | ------------------------------------------ | ------------------------------ | ---------------------------------- |\n",
    "| **StandardScaler** | Media = 0, desviación estándar = 1         | Datos normalmente distribuidos | `StandardScaler().fit_transform()` |\n",
    "| **MinMaxScaler**   | Escala entre 0 y 1                         | Redes neuronales o imágenes    | `MinMaxScaler().fit_transform()`   |\n",
    "| **RobustScaler**   | Usa mediana y IQR (no sensible a outliers) | Datos con muchos outliers      | `RobustScaler().fit_transform()`   |\n",
    "| **Normalizer**     | Normaliza filas (norma L2 = 1)             | Series temporales, clustering  | `Normalizer().fit_transform()`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5a44b",
   "metadata": {},
   "source": [
    "🎛️ Tipos de escalado (los más usados)\n",
    "### [__] 1. StandardScaler (Estandarización)\n",
    "\n",
    "    Lo que hace: centra los datos en media cero y los escala con varianza uno. Esto quiere decir que convierte tus datos a una distribución estándar (z-score).\n",
    "\n",
    "    Fórmula:\n",
    "    z=x−μσ\n",
    "    z=σx−μ​\n",
    "\n",
    "    donde μ es la media y σ la desviación estándar.\n",
    "\n",
    "    Cuándo usarlo:\n",
    "\n",
    "        Cuando tus datos están cercanos a una distribución normal (campana).\n",
    "\n",
    "        En modelos lineales (regresión lineal, logística), PCA, SVM, redes neuronales.\n",
    "\n",
    "    Cuidado: no es robusto frente a outliers. Los valores extremos distorsionan la media y σ.\n",
    "\n",
    "    🧠 Consejo: Antes de usarlo, puedes graficar un histograma o sns.kdeplot para verificar la forma de la distribución.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69abf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que 'X' son tus características y 'y' tu objetivo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajusta el escalador SOLO con los datos de entrenamiento\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transforma los datos de prueba con el escalador ya ajustado\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# El resultado es un array de NumPy, puedes convertirlo de nuevo a DataFrame si lo deseas\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9099e7",
   "metadata": {},
   "source": [
    "### [__]  2. MinMaxScaler (Normalización)\n",
    "\n",
    "    Lo que hace: escala todos los valores en un rango definido, usualmente entre 0 y 1.\n",
    "\n",
    "    Fórmula:\n",
    "    x′=x−min⁡(x)max⁡(x)−min⁡(x)\n",
    "    x′=max(x)−min(x)x−min(x)​\n",
    "\n",
    "    Cuándo usarlo:\n",
    "\n",
    "        En modelos basados en gradientes como redes neuronales (MLP, CNN, etc.).\n",
    "\n",
    "        En datos de imágenes (por ejemplo, pixel values de 0 a 255 → escalar a 0-1).\n",
    "\n",
    "        Cuando necesitas mantener la forma de la distribución, pero limitar el rango.\n",
    "\n",
    "    Cuidado: es muy sensible a outliers porque usa los valores extremos para definir los límites del rango.\n",
    "\n",
    "    🧠 Consejo: Si usas MinMaxScaler, asegúrate de haber tratado outliers antes (con IQR, winsorización, etc.).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47371997",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # El rango es personalizable\n",
    "\n",
    "# Ajusta y transforma de la misma manera que StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074823a",
   "metadata": {},
   "source": [
    "### [__]  3. RobustScaler\n",
    "\n",
    "    Lo que hace: utiliza la mediana y el rango intercuartílico (IQR) en lugar de la media y desviación estándar. Esto hace que sea resistente a valores extremos.\n",
    "\n",
    "    Fórmula:\n",
    "    x′=x−medianaIQR\n",
    "    x′=IQRx−mediana​\n",
    "\n",
    "    Cuándo usarlo:\n",
    "\n",
    "        Cuando sabes que tu dataset tiene outliers fuertes que no deseas eliminar.\n",
    "\n",
    "        En análisis financiero, donde los datos suelen tener colas largas o picos de valores.\n",
    "\n",
    "    🧠 Consejo: Muy útil como paso previo a modelos de ML donde no puedes permitir que los outliers distorsionen tu resultado, pero no puedes o no quieres eliminarlos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Ajusta y transforma\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41ef29",
   "metadata": {},
   "source": [
    "### [__]  4. Normalizer\n",
    "\n",
    "    Lo que hace: normaliza cada fila del dataset en lugar de cada columna. Se usa la norma L1 o L2 para hacer que el vector tenga una longitud de 1.\n",
    "\n",
    "    Ejemplo: útil para datos donde cada fila representa un vector, como en texto (TF-IDF), series temporales o clustering de comportamiento.\n",
    "\n",
    "    Cuándo usarlo:\n",
    "\n",
    "        En algoritmos que dependen de la dirección del vector más que de su magnitud, como clustering, técnicas de texto, etc.\n",
    "\n",
    "    ⚠️ Consejo: No confundas Normalizer con MinMaxScaler. Uno normaliza por fila, otro por columna.\n",
    "\n",
    "Escala cada fila (es decir, cada muestra) para que su norma (longitud del vector) sea igual a 1.\n",
    "\n",
    "Se usa en algoritmos que trabajan con dirección del vector más que con su magnitud:\n",
    "\n",
    "    Similaridad de texto (TF-IDF, Word embeddings)\n",
    "\n",
    "    Clustering (KMeans)\n",
    "\n",
    "    Datos temporales o de sensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7371133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos un DataFrame con 3 características por muestra\n",
    "data = np.array([\n",
    "    [3.0, 4.0, 0.0],\n",
    "    [1.0, 2.0, 2.0],\n",
    "    [0.0, 0.0, 10.0]\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['feature1', 'feature2', 'feature3'])\n",
    "\n",
    "# Inicializar el normalizador con norma L2 (por defecto)\n",
    "normalizer = Normalizer(norm='l2')  # También puedes usar 'l1' o 'max'\n",
    "\n",
    "# Aplicar normalización por fila\n",
    "data_normalized = normalizer.fit_transform(df)\n",
    "\n",
    "# Convertir a DataFrame para ver resultados\n",
    "df_normalizado = pd.DataFrame(data_normalized, columns=df.columns)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df)\n",
    "print(\"\\nNormalizado (L2):\")\n",
    "print(df_normalizado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7f6fc",
   "metadata": {},
   "source": [
    "¿Cómo escalar correctamente un dataset?\n",
    "\n",
    "    Trata los NaN antes. El escalador no puede trabajar con valores faltantes.\n",
    "\n",
    "    Separa tu dataset en entrenamiento y prueba antes de escalar.\n",
    "\n",
    "    Ajusta (fit) el escalador solo con el set de entrenamiento.\n",
    "\n",
    "    Transforma (transform) tanto entrenamiento como prueba con ese mismo escalador.\n",
    "\n",
    "    Guarda el escalador (si el modelo va a producción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea354ef",
   "metadata": {},
   "source": [
    "🚫 Errores comunes\n",
    "\n",
    "    Escalar después de entrenar el modelo (esto genera data leakage).\n",
    "\n",
    "    Escalar sin eliminar o tratar outliers (afecta a StandardScaler y MinMaxScaler).\n",
    "\n",
    "    Escalar variables categóricas convertidas a dummies (one-hot), cuando no hace falta.\n",
    "\n",
    "    Aplicar fit_transform() al set de prueba. ¡Solo se usa transform()!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14d2aa",
   "metadata": {},
   "source": [
    "🧠 Consejos prácticos\n",
    "\n",
    "    En proyectos de IA o deep learning, usa MinMaxScaler o normaliza a [-1, 1] para que la red converja mejor.\n",
    "\n",
    "    En modelos financieros o con valores dispersos, prueba primero con RobustScaler.\n",
    "\n",
    "    Si vas a usar PCA o clustering, escalado previo es obligatorio para que no domine una variable.\n",
    "\n",
    "    Si no sabes qué usar, prueba con StandardScaler y compara resultados.\n",
    "\n",
    "    Guarda tu escalador con joblib.dump(scaler, 'nombre_scaler.pkl') para reproducibilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e9470",
   "metadata": {},
   "source": [
    "| Punto                             | Estandarización            | Normalización                                                 |\n",
    "| --------------------------------- | -------------------------- | ------------------------------------------------------------- |\n",
    "| Centrado en media                 | ✅ Sí                       | ❌ No                                                          |\n",
    "| Rango controlado (ej: 0 a 1)      | ❌ No                       | ✅ Sí                                                          |\n",
    "| Forma de distribución se mantiene | ❌ No (transforma la forma) | ✅ Sí (solo cambia el rango)                                   |\n",
    "| Robusto contra outliers           | ❌ No                       | ❌ No (ambos son sensibles, usar RobustScaler si hay outliers) |\n",
    "| Escenarios típicos                | PCA, SVM, regresión        | Deep Learning, imágenes, magnitudes distintas                 |\n",
    "| Afecta unidades                   | Sí                         | Sí                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80944da7",
   "metadata": {},
   "source": [
    "---\n",
    "## [___] 8. Guardar archivo limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a470535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/processed/dataset_limpio.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "program_exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
