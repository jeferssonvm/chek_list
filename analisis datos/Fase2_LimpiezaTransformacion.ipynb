{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bf05b3",
   "metadata": {},
   "source": [
    "# Fase 2: Limpieza y Transformaci√≥n de Datos\n",
    "\n",
    "Objetivo: Corregir los problemas identificados en la Fase 1 y enriquecer el dataset para prepararlo para el modelado. La calidad y el rendimiento de tu futuro modelo dependen casi por completo de la calidad de esta fase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681d84f",
   "metadata": {},
   "source": [
    "### Manejo de nulos\n",
    "* **C√≥digo de Ejemplo:** `df['col'].fillna(df['col'].median())` o `df.dropna()`\n",
    "* **Consejo Profesional:** Usa la mediana si hay valores at√≠picos (outliers). Eliminar filas o columnas es el √∫ltimo recurso.\n",
    "\n",
    "---\n",
    "\n",
    "### Conversi√≥n de tipos\n",
    "* **C√≥digo de Ejemplo:** `df['fecha'] = pd.to_datetime(df['fecha'])`\n",
    "* **Consejo Profesional:** Los identificadores (IDs) num√©ricos deber√≠an tratarse como texto (strings) para evitar c√°lculos incorrectos.\n",
    "\n",
    "---\n",
    "\n",
    "### Eliminar duplicados\n",
    "* **C√≥digo de Ejemplo:** `df.drop_duplicates(inplace=True)`\n",
    "* **Consejo Profesional:** Revisa los duplicados antes de eliminarlos, ya que su presencia puede indicar errores en el proceso de recolecci√≥n de datos.\n",
    "\n",
    "---\n",
    "\n",
    "### Detecci√≥n y tratamiento de outliers\n",
    "* **C√≥digo de Ejemplo:**\n",
    "    ```python\n",
    "    Q1 = df['col'].quantile(0.25)\n",
    "    Q3 = df['col'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[df['col'] < Q3 + 1.5*IQR]\n",
    "    ```\n",
    "* **Consejo Profesional:** Visualiza los datos con un `boxplot` de Seaborn (`sns.boxplot`). Otras t√©cnicas incluyen la transformaci√≥n logar√≠tmica o la winsorizaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "### Ingenier√≠a de variables\n",
    "* **C√≥digo de Ejemplo:** `df['volumen_total'] = df['sets'] * df['reps']` o `df['dia_semana'] = df['fecha'].dt.dayofweek`\n",
    "* **Consejo Profesional:** En este paso es donde aplicas tu conocimiento del negocio para crear nuevas caracter√≠sticas relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "### Codificar categ√≥ricas\n",
    "* **C√≥digo de Ejemplo:** `pd.get_dummies(df, columns=['cat1'], drop_first=True)`\n",
    "* **Consejo Profesional:** Usa `drop_first=True` para evitar la multicolinealidad. Para variables con un orden inherente (ordinales), considera usar `OrdinalEncoder`.\n",
    "\n",
    "---\n",
    "\n",
    "### Escalado de datos\n",
    "* **C√≥digo de Ejemplo:**\n",
    "    ```python\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    ```\n",
    "* **Consejo Profesional:** Tambi√©n puedes considerar otras t√©cnicas como `MinMaxScaler` (para escalar a un rango espec√≠fico) o `RobustScaler` (si hay outliers).\n",
    "\n",
    "---\n",
    "\n",
    "### Guardar datos procesados\n",
    "* **C√≥digo de Ejemplo:** `df.to_csv('data/processed/datos_limpios.csv', index=False)`\n",
    "* **Consejo Profesional:** Usa `index=False` para evitar que el √≠ndice del DataFrame se guarde como una columna en el archivo CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20757453",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## [__] verificar nombres de columnas \n",
    "## [__] Identificar columnas con un solo valor\n",
    "## [__] datos a minuscula\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273d80d",
   "metadata": {},
   "source": [
    "## [__] 1. Eliminar duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25062f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [   ]Calcula el n√∫mero de duplicados antes de eliminarlos.\n",
    "dups_before = df_clean.duplicated().sum()\n",
    "\n",
    "#[    ] Elimina las filas duplicadas, manteniendo la primera aparici√≥n.\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n",
    "#Consejo Profesional: A veces, las filas pueden estar duplicadas\n",
    "# solo en un subconjunto de columnas clave (ej., id_cliente y \n",
    "# fecha_compra). Puedes especificar estas columnas con el par√°metro\n",
    "# subset: df_clean.drop_duplicates(subset=['id_cliente', \n",
    "# 'fecha_compra'], inplace=True).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b38699",
   "metadata": {},
   "source": [
    "## [__] Revisar duplicados parciales : mismo nombre y correo pero diferente ID\n",
    "## [__] Revisar claves primarias  ¬øhay valores repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72049c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [__] 2. Eliminar o imputar valores nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [   ] --- Estrategia 1: Imputaci√≥n para columnas num√©ricas ---\n",
    "# Si la distribuci√≥n es sesgada (como viste en el histograma), la mediana es m√°s robusta que la media.\n",
    "median_value = df_clean['columna_numerica_con_nulos'].median()\n",
    "df_clean['columna_numerica_con_nulos'].fillna(median_value, inplace=True)\n",
    "\n",
    "# [   ] --- Estrategia 2: Imputaci√≥n para columnas categ√≥ricas ---\n",
    "# La moda (el valor m√°s frecuente) es la mejor opci√≥n para imputar categor√≠as.\n",
    "mode_value = df_clean['columna_categorica_con_nulos'].mode()[0]\n",
    "df_clean['columna_categorica_con_nulos'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# [   ] --- Estrategia 3: Eliminaci√≥n de filas ---\n",
    "# √ösalo como √∫ltimo recurso si una fila tiene demasiados datos importantes faltantes.\n",
    "# 'subset' especifica la columna a revisar para la eliminaci√≥n.\n",
    "df_clean.dropna(subset=['columna_clave_con_nulos'], inplace=True)\n",
    "\n",
    "#[  ]Consejo Profesional: La decisi√≥n de imputar o eliminar \n",
    "# depende del contexto del negocio y la cantidad de datos \n",
    "# faltantes. Si una columna tiene >50% de nulos, considera \n",
    "# eliminar la columna por completo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e454492",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [__] 3. Correcci√≥n de Tipos de Datos (Dtypes)\n",
    "Meta: Asegurarse de que cada columna tenga el tipo de dato correcto para poder realizar operaciones y an√°lisis adecuados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[____] --- Convertir a tipo Fecha (datetime) ---\n",
    "# Es fundamental para poder realizar operaciones de series de tiempo.\n",
    "# 'errors='coerce'' convertir√° las fechas no v√°lidas en NaT (Not a Time),\n",
    "# que puedes manejar despu√©s.\n",
    "df_clean['columna_fecha'] = pd.to_datetime(df_clean['columna_fecha'], errors='coerce')\n",
    "\n",
    "\n",
    "# [___]--- Convertir a tipo Categ√≥rico (category) ---\n",
    "# Es m√°s eficiente en memoria que el tipo 'object' para columnas con un n√∫mero \n",
    "# limitado de valores √∫nicos.\n",
    "df_clean['columna_a_categoria'] = df_clean['columna_a_categoria'].astype('category') \n",
    "\n",
    "\n",
    "# [___] --- Convertir a tipo Num√©rico (int, float) ---\n",
    "# √ötil si tienes n√∫meros almacenados como texto (ej. '$1,200.50').\n",
    "# Primero, necesitas limpiar los caracteres no num√©ricos.\n",
    "df_clean['columna_dinero'] = df_clean['columna_dinero'].replace({'\\$': '', ',': ''}, regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33f139",
   "metadata": {},
   "source": [
    "### [__] Eliminar simbolos no numericos\n",
    "### [__] Detectar valores negativos donde no deberia\n",
    "### [__] elimianr caracteres especiales si es posible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ea7b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [__] 4. Corregir formatos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = df['col'].str.strip().str.lower().str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1f808",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [___] 5. Manejo de Outliers en An√°lisis de Datos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5450a1",
   "metadata": {},
   "source": [
    "### [__] Detecci√≥n de Outliers\n",
    "    M√©todos:\n",
    "    --    * Boxplot / Gr√°ficos\n",
    "       * Z-Score\n",
    "            ¬øQu√© es? Calcula cu√°ntas desviaciones est√°ndar se aleja un punto de la media. Un umbral com√∫n es un Z-score de +/- 3.\n",
    "            Ideal para: Datos que siguen una distribuci√≥n normal (Gaussiana). Es sensible a los propios outliers que inflan la media y la desviaci√≥n est√°ndar.\n",
    "        * IQR\n",
    "                ¬øQu√© es? Un outlier es cualquier valor que cae fuera del siguiente rango: [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR].\n",
    "                Ideal para: Distribuciones asim√©tricas o cuando no quieres asumir una distribuci√≥n normal. Es el m√©todo m√°s com√∫n y robusto.\n",
    "        Distribuci√≥n y visualizaci√≥n\n",
    "        MAD (desviaci√≥n absoluta de la mediana)\n",
    "            se utiliza para medir la dispersi√≥n de un conjunto de datos, especialmente cuando se busca una medida robusta que no sea sensible a valores at√≠picos o extremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#[__] Visualizar\n",
    "sns.boxplot(data=df[['columna1', 'columna2']])\n",
    "plt.title('Boxplot para detectar outliers')\n",
    "plt.show()\n",
    "\n",
    "# [__] Z-Score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include='number')))\n",
    "df_outliers_z = df[(z_scores > 3).any(axis=1)]\n",
    "\n",
    "\n",
    "#[__] IQR\n",
    "Q1 = df['columna'].quantile(0.25)\n",
    "Q3 = df['columna'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lim_inf = Q1 - 1.5 * IQR\n",
    "lim_sup = Q3 + 1.5 * IQR\n",
    "outliers_iqr = df[(df['columna'] < lim_inf) | (df['columna'] > lim_sup)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96a4dd",
   "metadata": {},
   "source": [
    "### [__] ‚úÇÔ∏è 2. Eliminaci√≥n de Outliers\n",
    "\n",
    "    Cu√°ndo usar:\n",
    "        Cuando el outlier es un error de medici√≥n evidente.\n",
    "        Cuando es poco probable que ese valor se repita.\n",
    "        Cuando distorsiona demasiado las m√©tricas estad√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR para eliminar\n",
    "df_filtrado = df[(df['columna'] >= lim_inf) & (df['columna'] <= lim_sup)]\n",
    "# Z-score para eliminar\n",
    "df_filtrado = df[(z_scores < 3).all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb09a1",
   "metadata": {},
   "source": [
    "‚úÖ Recomendaciones:\n",
    "*    Documenta cu√°ntos valores se eliminaron.\n",
    "*    Eval√∫a el impacto en el tama√±o del dataset.\n",
    "*    Guarda copia del dataset original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76141cc",
   "metadata": {},
   "source": [
    "### [___] üîÅ 3. Reemplazo (Winsorizaci√≥n)\n",
    "\n",
    "üìå Objetivo: Sustituir outliers por valores extremos v√°lidos sin perder la fila.\n",
    "\n",
    "* Qu√© es? Se \"aplanan\" los outliers. Cualquier valor por encima del percentil 95 (por ejemplo) se reemplaza por el valor del percentil 95. Lo mismo para el extremo inferior (ej. percentil 5).\n",
    "* Cu√°ndo usarla? Cuando crees que los outliers son leg√≠timos pero su magnitud extrema est√° afectando negativamente al modelo (especialmente modelos lineales). Conservas la idea de que es un valor \"alto\" o \"bajo\" sin que su escala domine.\n",
    "\n",
    "Cu√°ndo usar:\n",
    "\n",
    "    Cuando quieres mantener todos los datos pero reducir la influencia de extremos.\n",
    "\n",
    "    Cuando el outlier puede ser real pero necesitas controlarlo estad√≠sticamente.\n",
    "\n",
    "‚úÖ Recomendaciones:\n",
    "\n",
    "    Winsorizar especialmente √∫til si vas a usar modelos sensibles (como regresi√≥n lineal).\n",
    "\n",
    "    Anota los l√≠mites usados (lim_inf, lim_sup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d410497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsoriza al 5% en cada extremo (deja el 90% de los datos centrales intactos)\n",
    "df['columna_winsorizada'] = winsorize(df['columna'], limits=[0.05, 0.05])\n",
    "\n",
    "df['columna'] = np.where(df['columna'] > lim_sup, lim_sup,\n",
    "                  np.where(df['columna'] < lim_inf, lim_inf, df['columna']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367d036",
   "metadata": {},
   "source": [
    "### [__] üîÑ 4. Imputaci√≥n de valores Mediana o Media\n",
    "\n",
    "üìå Objetivo: Sustituir outliers por valores estad√≠sticos como la media o mediana.\n",
    "\n",
    "* ¬øQu√© es? Se sustituye el valor at√≠pico por la mediana (m√°s robusta) o la media de la columna.\n",
    "* ¬øCu√°ndo usarla? Cuando no quieres perder los datos de las otras columnas de esa fila. Es un m√©todo simple y r√°pido. Prefiere la mediana sobre la media si la distribuci√≥n es asim√©trica.\n",
    "\n",
    "Cu√°ndo usar:\n",
    "\n",
    "    Cuando los outliers son errores pero no puedes eliminar datos.\n",
    "\n",
    "    Si los datos son escasos o cr√≠ticos y no quieres perder informaci√≥n\n",
    "\n",
    " Recomendaciones:\n",
    "\n",
    "    Usa la mediana en vez de la media si los datos est√°n sesgados.\n",
    "\n",
    "    Documenta los valores imputados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediana = df['columna'].median()\n",
    "df['columna_imputada'] = df['columna'].apply(lambda x: mediana if x < limite_inferior or x > limite_superior else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283babd1",
   "metadata": {},
   "source": [
    "### [__] üîÅ 5. Transformaciones de datos\n",
    "\n",
    "Aplicas una funci√≥n matem√°tica a toda la variable para reducir el impacto de los outliers.\n",
    "\n",
    "¬øQu√© es? Funciones como la logar√≠tmica, ra√≠z cuadrada o Box-Cox comprimen la escala de la variable, acercando los outliers al resto de los datos.\n",
    "\n",
    "¬øCu√°ndo usarla?\n",
    "\n",
    "        Cuando la variable tiene una fuerte asimetr√≠a positiva (una cola larga a la derecha).\n",
    "\n",
    "        En modelos que asumen normalidad o linealidad (como la regresi√≥n lineal).\n",
    "\n",
    "        Cuando los outliers son una caracter√≠stica intr√≠nseca de la distribuci√≥n (ej. ingresos, tama√±os de empresas).\n",
    "\n",
    "‚úÖ Recomendaciones:\n",
    "\n",
    "    Usa log1p si hay ceros.\n",
    "\n",
    "    Visualiza despu√©s de transformar (histplot, boxplot).\n",
    "\n",
    "    No olvides destransformar al interpretar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logaritmo\n",
    "df['columna_log'] = np.log1p(df['columna'])\n",
    "\n",
    "# Ra√≠z cuadrada\n",
    "df['columna_sqrt'] = np.sqrt(df['columna'])\n",
    "\n",
    "# Box-Cox (requiere valores > 0)\n",
    "from scipy.stats import boxcox\n",
    "df['columna_boxcox'], _ = boxcox(df['columna'] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac4d68",
   "metadata": {},
   "source": [
    "### [__] üõ°Ô∏è 6. Uso de Modelos Robustecidos\n",
    "üìå Objetivo: Modelar sin eliminar ni transformar outliers directamente.\n",
    "üí° Cu√°ndo usar:\n",
    "\n",
    "    Cuando los outliers son inevitables.\n",
    "\n",
    "    Cuando usas modelos resistentes como √°rboles o medianas.\n",
    "\n",
    "üíª Modelos comunes:\n",
    "\n",
    "    RandomForest, XGBoost, GradientBoosting\n",
    "\n",
    "    HuberRegressor, RANSACRegressor (de sklearn.linear_model)\n",
    "\n",
    "    IsolationForest (para detecci√≥n autom√°tica)\n",
    "\n",
    "Modelos Robustos:\n",
    "\n",
    "    √Årboles de Decisi√≥n y ensambles (Random Forest, Gradient Boosting): Son naturalmente robustos a los outliers porque dividen el espacio de caracter√≠sticas en regiones y no se ven afectados por la magnitud de los valores.\n",
    "\n",
    "    Regresiones Robustas (ej. RANSAC, Huber Regressor): Modelos lineales que ponderan menos los errores grandes, reduciendo la influencia de los outliers.\n",
    "\n",
    "    Recomendaci√≥n: Si eliges esta v√≠a, simplemente alimenta los datos originales (o con m√≠nima limpieza) a uno de estos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844cb44",
   "metadata": {},
   "source": [
    "### [__] 7. An√°lisis Individual o Validaci√≥n del Outlier\n",
    "üìå Objetivo: Determinar si un outlier es realmente un error o un caso especial.\n",
    "üí° Cu√°ndo usar:\n",
    "\n",
    "    Cuando un valor parece raro pero puede ser leg√≠timo.\n",
    "\n",
    "    En √°reas sensibles como medicina, fraude, accidentes, etc.\n",
    "\n",
    "‚úÖ Recomendaciones:\n",
    "\n",
    "    Revisa el caso completo: ¬øotros campos est√°n bien?\n",
    "\n",
    "    Consulta con el dominio experto si es posible.\n",
    "\n",
    "    Nunca elimines autom√°ticamente sin investigar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3425",
   "metadata": {},
   "source": [
    "### [___] 8. Documentaci√≥n del tratamiento\n",
    "\n",
    "    üîé ¬øC√≥mo se detectaron?\n",
    "\n",
    "    üîÅ ¬øQu√© m√©todo se aplic√≥?\n",
    "\n",
    "    üìâ ¬øCu√°ntos valores se afectaron?\n",
    "\n",
    "    üßæ ¬øQu√© decisiones se tomaron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780944a",
   "metadata": {},
   "source": [
    "---\n",
    "## [___] 6. Crear variables √∫tiles (feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duracion_total'] = df['dias'] * df['horas_dia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007e0c5",
   "metadata": {},
   "source": [
    "| T√©cnica                          | Descripci√≥n breve                                            | Ejemplo de c√≥digo                                                       | ¬øCu√°ndo usar?                                             |\n",
    "| -------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| üî¢ Transformaci√≥n matem√°tica     | Aplica log, ra√≠z, potencias para normalizar o ajustar escala | `df['log_precio'] = np.log1p(df['precio'])`                             | Cuando hay sesgo o colas largas                           |\n",
    "| ‚ûó Variables combinadas/derivadas | Combina columnas para crear relaciones √∫tiles                | `df['precio_m2'] = df['precio'] / df['area']`                           | Cuando dos columnas est√°n relacionadas                    |\n",
    "| üîÅ Encoding categ√≥rico           | Convierte texto a n√∫meros                                    | `pd.get_dummies(df, columns=['genero'])`                                | Al usar modelos que requieren datos num√©ricos             |\n",
    "| üìÜ Variables de fecha            | Extrae partes de una fecha o calcula duraci√≥n                | `df['mes'] = df['fecha'].dt.month`                                      | Si tienes fechas (ventas, eventos, registros, etc.)       |\n",
    "| üî¢ Frecuencia o conteo           | Crea una nueva variable con la frecuencia de ocurrencia      | `df['freq'] = df['ciudad'].map(df['ciudad'].value_counts())`            | Para capturar importancia o rareza de categor√≠as          |\n",
    "| üß± Binning / Discretizaci√≥n      | Convierte valores continuos a rangos                         | `pd.cut(df['edad'], bins=[0,18,35,60], labels=[...])`                   | Para segmentar poblaciones o estabilizar valores extremos |\n",
    "| ‚úÖ Indicadores (flags)            | Crea columnas booleanas seg√∫n condiciones                    | `df['es_vip'] = df['gasto'] > 100000`                                   | Para marcar eventos o atributos clave                     |\n",
    "| üìä Estad√≠sticas por grupo        | Calcula medias, desv√≠os por grupo                            | `df['media_ciudad'] = df.groupby('ciudad')['precio'].transform('mean')` | Para contextualizar los datos dentro de grupos            |\n",
    "| ‚úñÔ∏è Interacciones entre variables | Multiplica o combina variables para capturar relaciones      | `df['edad_x_ingresos'] = df['edad'] * df['ingresos']`                   | Cuando sospechas relaciones no lineales o sin√©rgicas      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558796a",
   "metadata": {},
   "source": [
    "----\n",
    "## [___] 7. Escalar datos (si es necesario)\n",
    "\n",
    "| Escenario                                                    | ¬øEscalar?                                           | ¬øQu√© tipo?                      |\n",
    "| ------------------------------------------------------------ | --------------------------------------------------- | ------------------------------- |\n",
    "| √Årboles (Decision Tree, Random Forest, XGBoost)              | ‚ùå No necesario                                      | Ninguno                         |\n",
    "| Regresiones lineales / log√≠sticas                            | ‚úÖ S√≠                                                | Estandarizaci√≥n o normalizaci√≥n |\n",
    "| Modelos de ML sensibles a distancia (KNN, SVM, PCA, K-means) | ‚úÖ S√≠                                                | Recomendado                     |\n",
    "| Redes neuronales (MLP, Deep Learning)                        | ‚úÖ Imprescindible                                    | Normalizaci√≥n (0-1)             |\n",
    "| Datos categ√≥ricos codificados (one-hot)                      | ‚ùå No                                                | -                               |\n",
    "| Datos con outliers                                           | ‚ö†Ô∏è Escalar con RobustScaler o usar m√©todos robustos | S√≠                              |\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "| Escalador          | ¬øC√≥mo funciona?                            | Cu√°ndo usarlo                  | C√≥digo                             |\n",
    "| ------------------ | ------------------------------------------ | ------------------------------ | ---------------------------------- |\n",
    "| **StandardScaler** | Media = 0, desviaci√≥n est√°ndar = 1         | Datos normalmente distribuidos | `StandardScaler().fit_transform()` |\n",
    "| **MinMaxScaler**   | Escala entre 0 y 1                         | Redes neuronales o im√°genes    | `MinMaxScaler().fit_transform()`   |\n",
    "| **RobustScaler**   | Usa mediana y IQR (no sensible a outliers) | Datos con muchos outliers      | `RobustScaler().fit_transform()`   |\n",
    "| **Normalizer**     | Normaliza filas (norma L2 = 1)             | Series temporales, clustering  | `Normalizer().fit_transform()`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5a44b",
   "metadata": {},
   "source": [
    "üéõÔ∏è Tipos de escalado (los m√°s usados)\n",
    "### [__] 1. StandardScaler (Estandarizaci√≥n)\n",
    "\n",
    "    Lo que hace: centra los datos en media cero y los escala con varianza uno. Esto quiere decir que convierte tus datos a una distribuci√≥n est√°ndar (z-score).\n",
    "\n",
    "    F√≥rmula:\n",
    "    z=x‚àíŒºœÉ\n",
    "    z=œÉx‚àíŒº‚Äã\n",
    "\n",
    "    donde Œº es la media y œÉ la desviaci√≥n est√°ndar.\n",
    "\n",
    "    Cu√°ndo usarlo:\n",
    "\n",
    "        Cuando tus datos est√°n cercanos a una distribuci√≥n normal (campana).\n",
    "\n",
    "        En modelos lineales (regresi√≥n lineal, log√≠stica), PCA, SVM, redes neuronales.\n",
    "\n",
    "    Cuidado: no es robusto frente a outliers. Los valores extremos distorsionan la media y œÉ.\n",
    "\n",
    "    üß† Consejo: Antes de usarlo, puedes graficar un histograma o sns.kdeplot para verificar la forma de la distribuci√≥n.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69abf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que 'X' son tus caracter√≠sticas y 'y' tu objetivo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajusta el escalador SOLO con los datos de entrenamiento\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transforma los datos de prueba con el escalador ya ajustado\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# El resultado es un array de NumPy, puedes convertirlo de nuevo a DataFrame si lo deseas\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9099e7",
   "metadata": {},
   "source": [
    "### [__]  2. MinMaxScaler (Normalizaci√≥n)\n",
    "\n",
    "    Lo que hace: escala todos los valores en un rango definido, usualmente entre 0 y 1.\n",
    "\n",
    "    F√≥rmula:\n",
    "    x‚Ä≤=x‚àímin‚Å°(x)max‚Å°(x)‚àímin‚Å°(x)\n",
    "    x‚Ä≤=max(x)‚àímin(x)x‚àímin(x)‚Äã\n",
    "\n",
    "    Cu√°ndo usarlo:\n",
    "\n",
    "        En modelos basados en gradientes como redes neuronales (MLP, CNN, etc.).\n",
    "\n",
    "        En datos de im√°genes (por ejemplo, pixel values de 0 a 255 ‚Üí escalar a 0-1).\n",
    "\n",
    "        Cuando necesitas mantener la forma de la distribuci√≥n, pero limitar el rango.\n",
    "\n",
    "    Cuidado: es muy sensible a outliers porque usa los valores extremos para definir los l√≠mites del rango.\n",
    "\n",
    "    üß† Consejo: Si usas MinMaxScaler, aseg√∫rate de haber tratado outliers antes (con IQR, winsorizaci√≥n, etc.).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47371997",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # El rango es personalizable\n",
    "\n",
    "# Ajusta y transforma de la misma manera que StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074823a",
   "metadata": {},
   "source": [
    "### [__]  3. RobustScaler\n",
    "\n",
    "    Lo que hace: utiliza la mediana y el rango intercuart√≠lico (IQR) en lugar de la media y desviaci√≥n est√°ndar. Esto hace que sea resistente a valores extremos.\n",
    "\n",
    "    F√≥rmula:\n",
    "    x‚Ä≤=x‚àímedianaIQR\n",
    "    x‚Ä≤=IQRx‚àímediana‚Äã\n",
    "\n",
    "    Cu√°ndo usarlo:\n",
    "\n",
    "        Cuando sabes que tu dataset tiene outliers fuertes que no deseas eliminar.\n",
    "\n",
    "        En an√°lisis financiero, donde los datos suelen tener colas largas o picos de valores.\n",
    "\n",
    "    üß† Consejo: Muy √∫til como paso previo a modelos de ML donde no puedes permitir que los outliers distorsionen tu resultado, pero no puedes o no quieres eliminarlos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Ajusta y transforma\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41ef29",
   "metadata": {},
   "source": [
    "### [__]  4. Normalizer\n",
    "\n",
    "    Lo que hace: normaliza cada fila del dataset en lugar de cada columna. Se usa la norma L1 o L2 para hacer que el vector tenga una longitud de 1.\n",
    "\n",
    "    Ejemplo: √∫til para datos donde cada fila representa un vector, como en texto (TF-IDF), series temporales o clustering de comportamiento.\n",
    "\n",
    "    Cu√°ndo usarlo:\n",
    "\n",
    "        En algoritmos que dependen de la direcci√≥n del vector m√°s que de su magnitud, como clustering, t√©cnicas de texto, etc.\n",
    "\n",
    "    ‚ö†Ô∏è Consejo: No confundas Normalizer con MinMaxScaler. Uno normaliza por fila, otro por columna.\n",
    "\n",
    "Escala cada fila (es decir, cada muestra) para que su norma (longitud del vector) sea igual a 1.\n",
    "\n",
    "Se usa en algoritmos que trabajan con direcci√≥n del vector m√°s que con su magnitud:\n",
    "\n",
    "    Similaridad de texto (TF-IDF, Word embeddings)\n",
    "\n",
    "    Clustering (KMeans)\n",
    "\n",
    "    Datos temporales o de sensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7371133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos un DataFrame con 3 caracter√≠sticas por muestra\n",
    "data = np.array([\n",
    "    [3.0, 4.0, 0.0],\n",
    "    [1.0, 2.0, 2.0],\n",
    "    [0.0, 0.0, 10.0]\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['feature1', 'feature2', 'feature3'])\n",
    "\n",
    "# Inicializar el normalizador con norma L2 (por defecto)\n",
    "normalizer = Normalizer(norm='l2')  # Tambi√©n puedes usar 'l1' o 'max'\n",
    "\n",
    "# Aplicar normalizaci√≥n por fila\n",
    "data_normalized = normalizer.fit_transform(df)\n",
    "\n",
    "# Convertir a DataFrame para ver resultados\n",
    "df_normalizado = pd.DataFrame(data_normalized, columns=df.columns)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df)\n",
    "print(\"\\nNormalizado (L2):\")\n",
    "print(df_normalizado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7f6fc",
   "metadata": {},
   "source": [
    "¬øC√≥mo escalar correctamente un dataset?\n",
    "\n",
    "    Trata los NaN antes. El escalador no puede trabajar con valores faltantes.\n",
    "\n",
    "    Separa tu dataset en entrenamiento y prueba antes de escalar.\n",
    "\n",
    "    Ajusta (fit) el escalador solo con el set de entrenamiento.\n",
    "\n",
    "    Transforma (transform) tanto entrenamiento como prueba con ese mismo escalador.\n",
    "\n",
    "    Guarda el escalador (si el modelo va a producci√≥n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea354ef",
   "metadata": {},
   "source": [
    "üö´ Errores comunes\n",
    "\n",
    "    Escalar despu√©s de entrenar el modelo (esto genera data leakage).\n",
    "\n",
    "    Escalar sin eliminar o tratar outliers (afecta a StandardScaler y MinMaxScaler).\n",
    "\n",
    "    Escalar variables categ√≥ricas convertidas a dummies (one-hot), cuando no hace falta.\n",
    "\n",
    "    Aplicar fit_transform() al set de prueba. ¬°Solo se usa transform()!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14d2aa",
   "metadata": {},
   "source": [
    "üß† Consejos pr√°cticos\n",
    "\n",
    "    En proyectos de IA o deep learning, usa MinMaxScaler o normaliza a [-1, 1] para que la red converja mejor.\n",
    "\n",
    "    En modelos financieros o con valores dispersos, prueba primero con RobustScaler.\n",
    "\n",
    "    Si vas a usar PCA o clustering, escalado previo es obligatorio para que no domine una variable.\n",
    "\n",
    "    Si no sabes qu√© usar, prueba con StandardScaler y compara resultados.\n",
    "\n",
    "    Guarda tu escalador con joblib.dump(scaler, 'nombre_scaler.pkl') para reproducibilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e9470",
   "metadata": {},
   "source": [
    "| Punto                             | Estandarizaci√≥n            | Normalizaci√≥n                                                 |\n",
    "| --------------------------------- | -------------------------- | ------------------------------------------------------------- |\n",
    "| Centrado en media                 | ‚úÖ S√≠                       | ‚ùå No                                                          |\n",
    "| Rango controlado (ej: 0 a 1)      | ‚ùå No                       | ‚úÖ S√≠                                                          |\n",
    "| Forma de distribuci√≥n se mantiene | ‚ùå No (transforma la forma) | ‚úÖ S√≠ (solo cambia el rango)                                   |\n",
    "| Robusto contra outliers           | ‚ùå No                       | ‚ùå No (ambos son sensibles, usar RobustScaler si hay outliers) |\n",
    "| Escenarios t√≠picos                | PCA, SVM, regresi√≥n        | Deep Learning, im√°genes, magnitudes distintas                 |\n",
    "| Afecta unidades                   | S√≠                         | S√≠                                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80944da7",
   "metadata": {},
   "source": [
    "---\n",
    "## [___] 8. Guardar archivo limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a470535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/processed/dataset_limpio.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "program_exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
